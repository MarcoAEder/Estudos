{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceitos de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que é Processamento de Linguagem Natural (NLP)? | Leonardo Ribeiro - https://www.youtube.com/watch?v=Kaz-osMcdxk\n",
    "\n",
    "#### <b> Anotações </b>\n",
    "\n",
    "<b> O que é NLP? </b> <br>\n",
    "Algoritmo/modelo de aprendizado profundo utilizado na análise/processamento de linguagem natural.\n",
    "\n",
    "Exemplos: Google tradutor, assistentes virtuais (Siri, Alexa, Google assistente).\n",
    "\n",
    "<b> Linguagem natural </b> <br>\n",
    "Linguagem de comunicação entre humanos.\n",
    "\n",
    "<b> NLP </b> <br>\n",
    "Uma área de IA.\n",
    "Objetivo: Fazer com que o computador entenda a linguagem natural.\n",
    "Muito utilizado para responder perguntas e/ou fazer sugestões.\n",
    "\n",
    "<b> Dificuldades em trabalhar com NLP </b>\n",
    "<ul>\n",
    "    <li> Ambiguidade; </li>\n",
    "    <ul>\n",
    "        <li> Contexto; </li>\n",
    "        <ul>\n",
    "            <li> Exemplos: \"Policiais matam homem com uma faca\". \"O banco está próximo (banco (assento) ou banco (instituição financeira))\". </li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    <li> Probabilística (previsão de palavras/termos); </li>\n",
    "    <ul>\n",
    "        <li> Exemplo: \"Vamos nadar na... (piscina, praia, rio, etc)\". </li>\n",
    "    </ul>\n",
    "    <li> Raciocínio sobre o mundo (realidade, acontecimentos sobre o mundo (contexto)). </li>\n",
    "    <ul>\n",
    "        <li> Exemplo: \"Você viu a tragédia de ontem?\".</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<b> Aplicações </b>\n",
    "<ul>\n",
    "    <li> Tradução automática; </li>\n",
    "    <li> Sugestão de palavras; </li>\n",
    "    <li> Correção automática; </li>\n",
    "    <li> Análise de sentimento; </li>\n",
    "    <li> Anúncios; </li>\n",
    "    <li> Sistemas de diálogo. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introdução ao Processamento de Linguagem Natural - https://www.youtube.com/watch?v=ivTIgsLUfc4\n",
    "\n",
    "#### <b> Anotações </b>\n",
    "\n",
    "<b> O que é NLP? </b> <br>\n",
    "NLP é uma área interdisciplinar na linguística com a computação que visa estudar como o computador recebe, interpreta/processa e reproduz a língua natural humana.\n",
    "\n",
    "<b> Usabilidade do NLP </b>\n",
    "<ul>\n",
    "\t<li> Contrução de chatbots; </li>\n",
    "\t<li> Análise de sentimentos; </li>\n",
    "\t<li> Tradução automática; </li>\n",
    "\t<li> Assistentes digitais; </li>\n",
    "\t<li> Corretores / resumos automáticos de texto. </li>\n",
    "</ul>\n",
    "\n",
    "<b> Pré-processamento </b> <br>\n",
    "Limpeza e preparação do texto <br>\n",
    "Pré-processar um texto é preparar ele para ser recebido pela máquina. <br>\n",
    "Esse passo é importante quando usamos algoritmos mais simples de machine learning, e deixa de ser tão necessário com algumas técnicas de deep learning (modelos robustos). <br>\n",
    "Nem toda tarefa requer o mesmo pré-processamento.\n",
    "\n",
    "<img src = \"bases/img_conceitos_nlp_vid_2.jpg\">\n",
    "\n",
    "<b> Feature Extraction </b> <br>\n",
    "Para usarmos um modelo precisamos de features: <b> uma forma estruturada de armazenar informações. </b> Porém, textos são um tipo de dado não estruturado, assim, é difícil para o computador entendê-los e analisá-los. <br>\n",
    "Por isso, realizamos a chamada feature extraction, ou seja, transformamos o texto em uma informação numérica de modo que seja possível utilizá-lo para alimentar um modelo. Umas das maneiras mais populares e simples de fazer isso é com Bag of Words (BoW). <br>\n",
    "\n",
    "<b> Bag of Words </b> <br>\n",
    "O que é? BoW é uma forma de representar o texto de acordo com a ocorrência das palavras nele. O \"saco de palavras\" recebe esse nome porque não leva em conta a ordem ou a estrutura das palavras no texto, apenas se ela aparece ou a frequência com que aparece nele. <br>\n",
    "Por exemplo, se a palava AULA aparece muito num texto, ela se torna mais central e importante para a máquina. Portanto, BoW pode ser um ótimo método para determinar as palavras significativas de um texto com base no número de vezes que ela é usada. <br>\n",
    "\n",
    "<b> Passos para aplicar o BoW </b> <br>\n",
    "<ul>\n",
    "\t<li> Selecionar os dados; </li>\n",
    "\t<li> Gerar o vocabulário; </li>\n",
    "\t<li> Formar vetores a partir do documento. </li>\n",
    "</ul>\n",
    "\n",
    "<b> TFIDF </b> <br>\n",
    "O que é? Cada palavra no documento recebe uma pontuação TF-IDF, feita multiplicando duas métricas diferentes:\n",
    "<ul>\n",
    "\t<li> TF = Term Frquency (a frequência do termo), que mede a frequência com que um termo ocorre num documento; </li>\n",
    "\t<li> IDF = Inverse Document Frequency (Inverso da frequência nos documentos), que mede o quão importante um termo é no contexto de todos os documentos. </li>\n",
    "</ul>\n",
    "\n",
    "<img src = \"bases/img_conceitos_nlp_vid_2.2.jpg\">\n",
    "\n",
    "<b> Modelos </b> <br>\n",
    "Acesso pelo link: https://github.com/turing-usp/conceitos-basicos-NLP <br>\n",
    "Ou pelo diretório: \"C:\\Users\\masilva11\\Documents\\Pessoal\\Estudos\\internet\\nlp\\conceitos-basicos-NLP\" <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aula 15 - Processamento de Linguagem Natural (PLN) | Curso Ciência de Dados e Machine Learning - https://www.youtube.com/watch?v=TjPyYOE_Khs\n",
    "\n",
    "#### <b> Anotações </b>\n",
    "\n",
    "<b> Usabilidade/Exemplos </b> <br>\n",
    "<ul>\n",
    "\t<li> GPT-3 - Geração de HTML + CSS (Geração de texto); </li>\n",
    "\t<li> AI Dungeon - Geração de histórias de RPG de Mesa (Geração de texto); </li>\n",
    "\t<li> IMDB PT - Classificação de reviews de filmes (Análise de sentimentos); </li>\n",
    "\t<li> Named Entity Recognition. </li>\n",
    "</ul>\n",
    "\n",
    "<b> Aquisição de dados (Corpus) </b> <br>\n",
    "<ul>\n",
    "\t<li> Scraping e crawling; </li>\n",
    "\t<ul>\n",
    "\t\t<li> Selenium, requests e beautiful soup. </li>\n",
    "\t</ul>\n",
    "\t<li> Datasets já conceituados academicamente; </li>\n",
    "\t<ul>\n",
    "\t\t<li> Bergen corpus of London teenage language (COLT), IMDB reviews, Stanford sentiment treebank, etc. </li>\n",
    "\t</ul>\n",
    "\t<li> Separação; </li>\n",
    "\t<ul>\n",
    "\t\t<li> Dados já se encontram num banco de dados SQL/Spark. </li>\n",
    "\t</ul>\n",
    "\t<li> Aquisição baseada em tarefa. </li>\n",
    "\t<ul>\n",
    "\t\t<li> Exemplo: Geração de texto vs. Análise de Toxicidade. No caso de um gerador de textos, removemos do modelo palavras e termos que sejam ofencivos para que o geração não os utilize durante a sua execução. Em contra partida, um analista de toxicidade precisa de modelos que sejam ofencivos para que ele possa discernir e trabalhar com estes dados. </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "<b> Cuidados </b> <br>\n",
    "Deve-se ter cuidado com a coleta e separação dos dados pois, em modelos de NLP é comum, principalmente em geradores de texto perpetuar esteriótipos, racismo entre outros. Assim como seres humanos, o modelo pode criar ou reforçar vieses de acordo com os dados que recebe.\n",
    "\n",
    "<b> Limpeza </b> <br>\n",
    "<ul>\n",
    "\t<li> Remoção de stopwords; </li>\n",
    "\t<ul>\n",
    "\t\t<li> \"<s> As </s> rodovias selecionadas <s> para </s> receber <s> o </s> sinal foram <s> as </s> consideradas estratégicas <s> para o </s> escoamento <s> da </s> produção agropecuária\". Existem diversas bibliotecas que podem ser utilizadas para a remoção de stopwords, exemplo: NLTK, Gensin e spaCy. </li>\n",
    "\t</ul>\n",
    "\t<li> Remoção de xingamentos e palavras de baixo calão </li>\n",
    "\t<ul>\n",
    "\t\t<li> Existem diversos datasets de vocabulários (palavrões) que pode ser utilizados para treinamentos de modelos. Exemplo: Our list of dirty, naughty, obscene and otherwise bag words. </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "<b> Pré-processamento </b> <br>\n",
    "<b> Bag-of-words </b> <br>\n",
    "<img src = \"bases/img_conceitos_nlp_vid_3.jpg\">\n",
    "\n",
    "<ul>\n",
    "\t<li> Vocabulário grande -> vetores grandes -> <i> Curse of dimensionality; </i> </li>\n",
    "\t<ul>\n",
    "\t\t<li> Tamanho do dicionário e \"espaçamento\" entre os termos. </li>\n",
    "\t</ul>\n",
    "\t<li> Necessidade de normalizar a importância das palavras; </li>\n",
    "\t<li> Não diferencia palavras comuns e palavras mais específicas. </li>\n",
    "\t<ul>\n",
    "\t\t<li> TF-IDF aborda extamente esse problema </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "<b> TF-IDF (Term Frequency-Inverse Document Frequency) </b> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.2\"> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.3\"> <br>\n",
    "\n",
    "<b> Ineficiências </b> <br>\n",
    "<b> Anotações </b> <br>\n",
    "Algumas ineficiências que o TF-IDF apresenta:\n",
    "<ul>\n",
    "\t<li> Ordem das palavras; </li>\n",
    "\t<ul>\n",
    "\t\t<li> As palavras não seguem ordem alguma, muito menos a ordem determinada pelo documento. </li>\n",
    "\t</ul>\n",
    "\t<li> Negação; </li>\n",
    "\t<ul>\n",
    "\t\t<li> Para o modelo negações não possuem relevância, porém no contexto de um texto a palavra tem relevância sim. Exemplo: \"Eu NÃO gosto de açaí\". </li>\n",
    "\t</ul>\n",
    "\t<li> Palavras correlatas; </li>\n",
    "\t<ul>\n",
    "\t\t<li> Palavras correlatas podem ser consideradas no modelo, ou seja, o modelo dará relevância para palavras diferentes mas com o mesmo sentido. Exemplo: educação, educadamente e educar. Uma solução para este problema é stemming, porém com as suas devidas observações. </li>\n",
    "\t</ul>\n",
    "\t<li> Duplo sentido. </li>\n",
    "\t<ul>\n",
    "\t\t<li> O modelo não consegue distinguir palvras que tenham duplo sentido. Exemplo: Banco (acento ou instituição financeira). </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "<b> Conteúdo slide </b> <br>\n",
    "<ul>\n",
    "\t<li> Vocabulário grande -> vetores grandes -> <i> Curse of dimensionality. </i> </li>\n",
    "\t<ul>\n",
    "\t\t<li> Representa o documento inteiro de uma vez; </li>\n",
    "\t\t<li> Representação literal tende a ser ineficiente. </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "<b> Word2Vec </b> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.4\"> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.5\"> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.6\"> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.7\"> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.8\"> <br>\n",
    "<img src = \"arquivos/img_conceitos_nlp_vid_3.9\"> <br>\n",
    "\n",
    "<b> Treinamento e avaliação </b> <br>\n",
    "<b> Treinamento </b>\n",
    "<ul>\n",
    "\t<li> Muitas tasks diferentes dificultam a escolha do modelo; </li>\n",
    "\t<li> Como os modelos que você está considerando se adaptam à task? </li>\n",
    "\t<ul>\n",
    "\t\t<li> Vale a pena consultar na literatura. </li>\n",
    "\t</ul>\n",
    "\t<li> Faça baselines! </li>\n",
    "\t<ul>\n",
    "\t\t<li> Canivetes suíços de PLN para Python: NLTK, Gensin e spaCy. </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "<b> Avaliação </b> <br>\n",
    "<ul>\n",
    "\t<li> Temos muitas tasks diferentes -> muitas métricas diferentes;</li>\n",
    "\t<li> Procurar na literatura é fundamental. </li>\n",
    "\t<ul>\n",
    "\t\t<li> BLEU para tradução, GLUE métrica geral dce entenfimento de linguagem, etc. </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "<b> Referências </b> <br>\n",
    "<ul>\n",
    "\t<li> Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 1 – Introduction and Word Vectors - https://www.youtube.com/watch?v=8rXD5-xhemo </li>\n",
    "\t<li> Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 2 – Word Vectors and Word Senses - https://www.youtube.com/watch?v=kEMJRjEdNzM </li>\n",
    "\t<li> The Illustrated Word2vec - https://jalammar.github.io/illustrated-word2vec/ </li>\n",
    "</ul>\n",
    "\n",
    "<b> Slide </b> <br>\n",
    "Acesso pelo link: https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUhaVlk1UEFyUGRFWkluUWcwV1hoVG5TSWZXZ3xBQ3Jtc0ttc1RHbUZwMC1PZVlSc0RveV9pQk14Qk1FS0k3ck1xOENLM1d3UXFLanFpcnBQOVowbWVwcjRqLUg0Zkw5cnRubmxRRnI2aU9CeDJKY1BIaWhkOHVzWU5ZVDh6RVlkZ2piR1IzZERpYmc5SW9xVUplZw&q=https%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F1OfAi9uj16A5mf2BMenM8KvOoDI24uauf_2reGN4_xNI%2Fedit%3Fusp%3Dsharing&v=TjPyYOE_Khs <br>\n",
    "Acesso no diretório: \"C:\\Users\\masilva11\\Documents\\Pessoal\\Estudos\\internet\\nlp\\arquivos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy em português - https://www.youtube.com/watch?v=1pGe5OSbbDg&list=PL4OAe-tL47saZwtt9fLHmT5cas57rjCCW&index=1\n",
    "\n",
    "#### <b> Anotações </b>\n",
    "\n",
    "<b> Vídeo 01 - Configurando o ambiente </b> <br>\n",
    "\n",
    "<b> Vídeo 02 - Aprendizado Supervisionado e Não supervisionado </b> <br>\n",
    "\n",
    "<b> Tipos de aprendizado </b>\n",
    "<ul>\n",
    "\t<li> Supervisionado </li>\n",
    "\t<li> Quando há noção de padrões, ou seja, há uma noção clara dos dados e o \"que é esperado\" pelo modelo (treinamento de modelos). </li>\n",
    "\t<ul>\n",
    "\t\t<li> Exemplo: Elaborar um modelo que consiga identificar o Faustão utilizando fotos dele. Inputs: 5k de fotos do Faustão e 3k de fotos NÃO Faustão. </li>\n",
    "\t</ul>\n",
    "\t<li> Não supervisionado </li>\n",
    "\t<ul>\n",
    "\t\t<li> Quando não há noção de padrões, ou seja, não há noção dos dados e seus tipos e esperamos que o modelo faça a classificação deles (elaboração de padrões entre os dados). </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "SpaCy é utilizado em modelos de aprendizado supervisionado. Como a questão de texto e linguagem natural possuir n variáveis, é necessario o supervisionamento e validação dos modelos de acordo com os dados utilizados. <br>\n",
    "\n",
    "<b> Vídeo 03 - Verificando um modelo em branco </b> <br>\n",
    "Instanciando e executando spaCy com um modelo em branco, ou sem qualquer tipo de aprendizado. <br>\n",
    "\n",
    "<b> Vídeo 04 - Pipelines e instalando modelos no spaCy </b> <br>\n",
    "As pipelines são etapas que compõem todo o processamento de texto. Exemplo: tokenizer - Separação de texto em tags; tagger - Prevê tags dentro do discurso; Lemmatizer - Determina a raiz da palavra; etc.\n",
    "\n",
    "Os modelos tem diferenças e possuem finalidades diferentes. Os modelos são classificados de três formas: Tipo - Definem cada recurso disponívle por modelo; Gênero - Tipo de texto que o modelo é treinado. Exemplo: textos da web ou notícias; Tamanho - É a quantidade de vetores que o modelo possui. Tipos: sm (small/pequeno), md (medium/médio), lg (large/grande) e trf. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraia palavras-chave usando spaCy em Python\n",
    "#### Referência:\n",
    "https://ichi.pro/pt/extraia-palavras-chave-usando-spacy-em-python-81931153149887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lengthy', 'alexander', 'father', 'greatest', 'greek', 'macedon', 'years', 'empires', 'africa', 'northeastern', 'campaign', 'age', 'kingdom', 'india', 'alexandros', 'bc', 'western', 'king', 'throne', 'commanders', 'greece', 'iii', 'ii', 'ancient', 'battle', 'military', 'history', 'june', 'successful', 'asia', 'great', 'largest', 'july', 'northwestern', 'undefeated', 'philip'}\n"
     ]
    }
   ],
   "source": [
    "## Importando bibliotecas\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "## Importando modelos.\n",
    "nlp = spacy.load(\"en_core_web_sm\")    ## Inglês.\n",
    "# nlp = spacy.load(\"pt_core_news_sm\")    ## Português.\n",
    "\n",
    "## Modelos de texto (EN) -- alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('arquivos/alexander_the_great.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "## Modelos de texto (PT-BR) -- alexandre_o_grande.txt ## alfredo_o_grande.txt ## forca_expedicionaria_brasileira.txt\n",
    "# file = open('bases/alexandre_o_grande.txt', 'r', encoding = 'utf8')\n",
    "# contents = file.readlines()\n",
    "\n",
    "def get_hotWords(text):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    ## Lista de classes gramaticais.\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN']    ## PROPN -- substantivo próprio, ADJ -- adjetivo e NOUN -- substantivo.\n",
    "    doc = nlp(text.lower())    ## Tokenizando texto.\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "            ## Se o token for uma stop word ou pontuação deve ser ignorada.\n",
    "            if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "                continue\n",
    "\n",
    "            ## Se o token for válido, deve ser armazernada na lista.\n",
    "            if(token.pos_ in pos_tag):\n",
    "                result.append(token.text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "## Executando função.\n",
    "listResults = get_hotWords(contents[0])\n",
    "\n",
    "## Resultados sem duplicados.\n",
    "listResults = set(listResults)\n",
    "print(listResults)\n",
    "\n",
    "## Transformando resultados em hastags - Classificação baseado em frequência (ideia do artigo).\n",
    "# hashtags = [('#' + x[0]) for x in Counter(listResults).most_common(5)]\n",
    "# print(' '.join(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four of the easiest and most effective methods to Extract Keywords from a Single Text using Python\n",
    "#### Referência:\n",
    "https://www.analyticsvidhya.com/blog/2022/01/four-of-the-easiest-and-most-effective-methods-of-keyword-extraction-from-a-single-text-using-python/#h2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAKE\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0020025519308588 <br>\n",
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/LIAAD/yake\n",
      "  Cloning https://github.com/LIAAD/yake to c:\\users\\masilva11\\appdata\\local\\temp\\pip-req-build-2llkowm2\n",
      "  Resolved https://github.com/LIAAD/yake to commit 238ae58c5ba39326a96862ee0e9cb817e5958440\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): still running...\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tabulate in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (0.8.9)\n",
      "Requirement already satisfied: click>=6.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (8.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (1.22.4)\n",
      "Requirement already satisfied: segtok in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (1.5.11)\n",
      "Requirement already satisfied: networkx in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (2.8.2)\n",
      "Requirement already satisfied: jellyfish in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from click>=6.0->yake==0.4.8) (0.4.4)\n",
      "Requirement already satisfied: regex in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from segtok->yake==0.4.8) (2022.4.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake 'C:\\Users\\masilva11\\AppData\\Local\\Temp\\pip-req-build-2llkowm2'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole text to be usedn VECTORIZATION OF TEXT USING DATA MINING METHODS, In the text mining tasks, textual representation should be not only efficient but also interpretable, as this enables an understanding of the operational logic underlying the data mining models. Traditional text vectorization methods such as TF-IDF and bag-of-words are effective and characterized by intuitive interpretability, but suffer from the «curse of dimensionality», and they are unable to capture the meanings of words. On the other hand, modern distributed methods effectively capture the hidden semantics, but they are computationally intensive, time-consuming, and uninterpretable. This article proposes a new text vectorization method called Bag of weighted Concepts BoWC that presents a document according to the concepts’ information it contains. The proposed method creates concepts by clustering word vectors (i.e. word embedding) then uses the frequencies of these concept clusters to represent document vectors. To enrich the resulted document representation, a new modified weighting function is proposed for weighting concepts based on statistics extracted from word embedding information. The generated vectors are characterized by interpretability, low dimensionality, high accuracy, and low computational costs when used in data mining tasks. The proposed method has been tested on five different benchmark datasets in two data mining tasks; document clustering and classification, and compared with several baselines, including Bag-of-words, TF-IDF, Averaged GloVe, Bag-of-Concepts, and VLAC. The results indicate that BoWC outperforms most baselines and gives 7% better accuracy on average\n"
     ]
    }
   ],
   "source": [
    "title = \"VECTORIZATION OF TEXT USING DATA MINING METHODS\"\n",
    "text = \"In the text mining tasks, textual representation should be not only efficient but also interpretable, as this enables an understanding of the operational logic underlying the data mining models. Traditional text vectorization methods such as TF-IDF and bag-of-words are effective and characterized by intuitive interpretability, but suffer from the «curse of dimensionality», and they are unable to capture the meanings of words. On the other hand, modern distributed methods effectively capture the hidden semantics, but they are computationally intensive, time-consuming, and uninterpretable. This article proposes a new text vectorization method called Bag of weighted Concepts BoWC that presents a document according to the concepts’ information it contains. The proposed method creates concepts by clustering word vectors (i.e. word embedding) then uses the frequencies of these concept clusters to represent document vectors. To enrich the resulted document representation, a new modified weighting function is proposed for weighting concepts based on statistics extracted from word embedding information. The generated vectors are characterized by interpretability, low dimensionality, high accuracy, and low computational costs when used in data mining tasks. The proposed method has been tested on five different benchmark datasets in two data mining tasks; document clustering and classification, and compared with several baselines, including Bag-of-words, TF-IDF, Averaged GloVe, Bag-of-Concepts, and VLAC. The results indicate that BoWC outperforms most baselines and gives 7% better accuracy on average\"\n",
    "full_text = title +\", \"+ text\n",
    "\n",
    "print(\"The whole text to be usedn\",full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrase:  operational logic underlying : score 0.008502958451052589\n",
      "Keyphrase:  text vectorization methods : score 0.015613284939549285\n",
      "Keyphrase:  text vectorization : score 0.02310717508615897\n",
      "Keyphrase:  Traditional text vectorization : score 0.02325791341228692\n",
      "Keyphrase:  data mining models : score 0.02830809004349318\n",
      "Keyphrase:  data mining tasks : score 0.033863083795882626\n",
      "Keyphrase:  DATA MINING : score 0.03618462463953267\n",
      "Keyphrase:  text mining tasks : score 0.037652251074155374\n",
      "Keyphrase:  enables an understanding : score 0.04036782511075581\n",
      "Keyphrase:  operational logic : score 0.04036782511075581\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "keywords = kw_extractor.extract_keywords(full_text)\n",
    "\n",
    "for kw, v in keywords:\n",
    "  print(\"Keyphrase: \",kw, \": score\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction Methods from Documents in NLP\n",
    "#### Referência: https://www.analyticsvidhya.com/blog/2022/03/keyword-extraction-methods-from-documents-in-nlp/#h2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake algorithm\n",
    "#### Referência: https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.6.2\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 8.6 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.4.24-cp38-cp38-win_amd64.whl (262 kB)\n",
      "     ------------------------------------- 262.1/262.1 kB 16.8 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: click in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (0.4.4)\n",
      "Installing collected packages: regex, joblib, nltk, rake-nltk\n",
      "Successfully installed joblib-1.1.0 nltk-3.7 rake-nltk-1.0.6 regex-2022.4.24\n"
     ]
    }
   ],
   "source": [
    "## INSTALAÇÃO.\n",
    "!pip3 install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11 june',\n",
       " '21 july',\n",
       " 'lengthy military',\n",
       " 'successful military',\n",
       " 'ruling years',\n",
       " 'father philip',\n",
       " 'ancient greek',\n",
       " '336 bc',\n",
       " 'widely considered',\n",
       " 'northwestern india',\n",
       " 'northeastern africa']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "## Modelos de texto (EN).\n",
    "## alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('bases/alexander_the_great.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "## Modelos de texto (PT-BR).\n",
    "## alexandre_o_grande.txt ## alfredo_o_grande.txt ## forca_expedicionaria_brasileira.txt\n",
    "# file = open('bases/alexandre_o_grande.txt', 'r', encoding = 'utf8')\n",
    "# contents = file.readlines()\n",
    "\n",
    "## Modelo antigo.\n",
    "# my_text = \"\"\"When it comes to evaluating the performance of keyword extractors, you can use some of the standard metrics in machine learning: accuracy, precision, recall, and F1 score. \\\n",
    "#   However, these metrics don’t reflect partial matches; they only consider the perfect match between an extracted segment and the correct prediction for that tag. \\\n",
    "#   Fortunately, there are some other metrics capable of capturing partial matches. An example of this is ROUGE.\"\"\"\n",
    "\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(contents[0]) # (my_text)    ## Extraindo palavras-chaves do texto.\n",
    "\n",
    "## Ranqueando palavras pela pontuação.\n",
    "rankedList = r.get_ranked_phrases_with_scores()    ## Lista de tuplas.\n",
    "\n",
    "keywordList = []\n",
    "for keyword in rankedList:\n",
    "\n",
    "  keyword_updated = keyword[1].split()\n",
    "  keyword_updated_string = \" \".join(keyword_updated[:2])\n",
    "  keywordList.append(keyword_updated_string)\n",
    "\n",
    "  if(len(keywordList) > 10):\n",
    "    break\n",
    "\n",
    "keywordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting summa\n",
      "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
      "     ---------------------------------------- 54.9/54.9 kB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from summa) (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from scipy>=0.19->summa) (1.22.4)\n",
      "Building wheels for collected packages: summa\n",
      "  Building wheel for summa (setup.py): started\n",
      "  Building wheel for summa (setup.py): finished with status 'done'\n",
      "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54390 sha256=5b47ebadff514820244f67c40315ca2fec67ceae2f22d318ab8819b043370047\n",
      "  Stored in directory: c:\\users\\masilva11\\appdata\\local\\pip\\cache\\wheels\\fd\\6a\\dd\\209eb19d5f2266b9cfd06827539bf70435b0ad5fe8244e52d3\n",
      "Successfully built summa\n",
      "Installing collected packages: summa\n",
      "Successfully installed summa-1.2.0\n",
      "[('methods', 0.2958531418898542), ('method', 0.2958531418898542), ('document', 0.2930064955472458), ('concepts', 0.25972098927238496), ('concept', 0.25972098927238496), ('mining', 0.20425273810869565), ('vectorization', 0.20080655873686834), ('word vectors', 0.18267366210822367), ('computationally', 0.16718186386765665), ('computational', 0.16718186386765665)]\n"
     ]
    }
   ],
   "source": [
    "# TextRank\n",
    "\n",
    "!pip install summa\n",
    "from summa import keywords\n",
    "\n",
    "TR_keywords = keywords.keywords(full_text, scores=True)\n",
    "print(TR_keywords[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificação do artigo\n",
    "#### Aplicando artigo para modelos em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho:  19 ['conhecida também', 'cobra está', 'segunda guerra', '1ª divisão', 'era formada', 'brasil participar', 'mais fácil', 'em suas', 'também batizada', 'rompimento da', 'e um', 'incluídos todos', 'seu lema', 'ofensiva aliada', 'força expedicionária', 'tal força', 'que seria', 'que durante', 'que']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "## Executando único modelo.\n",
    "## Modelos de texto (PT-BR).\n",
    "arquivo = open('bases/forca_expedicionaria_brasileira.txt', 'r', encoding = 'utf8')    ## alexandre_o_grande.txt ## alfredo_o_grande.txt ## forca_expedicionaria_brasileira.txt\n",
    "conteudo = arquivo.readlines()\n",
    "\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(conteudo[0])    ## Extraindo palavras-chaves do texto.\n",
    "\n",
    "## Ranqueando palavras pela pontuação.\n",
    "rankedList = r.get_ranked_phrases_with_scores()    ## Lista de tuplas.\n",
    "\n",
    "keywordList = []\n",
    "for keyword in rankedList:\n",
    "\n",
    "  keyword_updated = keyword[1].split()\n",
    "  keyword_updated_string = \" \".join(keyword_updated[:2])\n",
    "  keywordList.append(keyword_updated_string)\n",
    "\n",
    "  ## Filtrando tamanho da lista de palavras.\n",
    "  # if(len(keywordList) > 10):\n",
    "    # break\n",
    "\n",
    "print(\"Tamanho: \", len(keywordList), keywordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "#### Referência: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wife',\n",
       " 'c.',\n",
       " 'anglo',\n",
       " 'england',\n",
       " 'military',\n",
       " 'change',\n",
       " 'aethelwulf',\n",
       " 'brothers',\n",
       " 'alfred',\n",
       " 'turn']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_hotwords(text):\n",
    "\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "\n",
    "    return result\n",
    "\n",
    "## Importando modelos.\n",
    "## alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('bases/alfred_the_great.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "## Modelo antigo.\n",
    "new_text = \"\"\"When it comes to evaluating the performance of keyword extractors, you can use some of the standard metrics in machine learning: accuracy, precision, recall, and F1 score. \\\n",
    "  However, these metrics don’t reflect partial matches; they only consider the perfect match between an extracted segment and the correct prediction for that tag. \\\n",
    "  Fortunately, there are some other metrics capable of capturing partial matches. An example of this is ROUGE.\"\"\"\n",
    "\n",
    "## Executando função.\n",
    "output = set(get_hotwords(contents[0])) ## (new_text))\n",
    "most_common_list = Counter(output).most_common(10)    ## \"Counter\" - Contador de itens. ## \"most_common\" - Ordena os valores pelo tamanho.\n",
    "\n",
    "## Inserindo resultados em lista.\n",
    "result = []\n",
    "for item in most_common_list:\n",
    "  result.append(item[0])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textrank\n",
    "#### Referência: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytextrank\n",
      "  Downloading pytextrank-3.2.3-py3-none-any.whl (30 kB)\n",
      "Collecting scipy>=1.7\n",
      "  Downloading scipy-1.8.1-cp38-cp38-win_amd64.whl (36.9 MB)\n",
      "     ---------------------------------------- 36.9/36.9 MB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments>=2.7.4 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pytextrank) (2.12.0)\n",
      "Collecting icecream>=2.1\n",
      "  Downloading icecream-2.1.2-py2.py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: spacy>=3.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pytextrank) (3.3.0)\n",
      "Collecting graphviz>=0.13\n",
      "  Downloading graphviz-0.20-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 47.0/47.0 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting networkx[default]>=2.6\n",
      "  Downloading networkx-2.8.2-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: executing>=0.3.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from icecream>=2.1->pytextrank) (0.8.3)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from icecream>=2.1->pytextrank) (0.4.4)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from icecream>=2.1->pytextrank) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from networkx[default]>=2.6->pytextrank) (1.22.4)\n",
      "Collecting matplotlib>=3.4\n",
      "  Downloading matplotlib-3.5.2-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 11.2 MB/s eta 0:00:00\n",
      "Collecting pandas>=1.3\n",
      "  Downloading pandas-1.4.2-cp38-cp38-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (8.0.16)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (62.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.7.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.9.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.0.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.2-cp38-cp38-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 55.4/55.4 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "     -------------------------------------- 930.9/930.9 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.1.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "     -------------------------------------- 503.5/503.5 kB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pathy>=0.3.5->spacy>=3.0->pytextrank) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=3.0->pytextrank) (4.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2022.5.18.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from jinja2->spacy>=3.0->pytextrank) (2.1.1)\n",
      "Installing collected packages: pytz, scipy, pillow, networkx, kiwisolver, graphviz, fonttools, cycler, pandas, matplotlib, icecream, pytextrank\n",
      "Successfully installed cycler-0.11.0 fonttools-4.33.3 graphviz-0.20 icecream-2.1.2 kiwisolver-1.4.2 matplotlib-3.5.2 networkx-2.8.2 pandas-1.4.2 pillow-9.1.1 pytextrank-3.2.3 pytz-2022.1 scipy-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytextrank\n",
    "# spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World War II', 'fighter squadron', 'Air Force', 'Cobras Fumantes', 'Força Expedicionária Brasileira', 'Allied forces', 'Expedicionária Brasileira', 'liaison flight', 'the Mediterranean Theatre of World War II', 'Allied']\n"
     ]
    }
   ],
   "source": [
    "import pytextrank\n",
    "import spacy\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "## Importando modelos.\n",
    "## alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('bases/brazilian_expeditionary_force.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(contents[0]) ## (text)\n",
    "\n",
    "result = []\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases[:10]:\n",
    "    # print(phrase.text)\n",
    "    result.append(phrase.text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBert\n",
    "#### Referência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.5.1.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sentence-transformers>=0.3.8\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "     ---------------------------------------- 79.7/79.7 kB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn>=0.22.2\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-win_amd64.whl (7.3 MB)\n",
      "     ---------------------------------------- 7.3/7.3 MB 15.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from keybert) (1.22.4)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-12.4.4-py3-none-any.whl (232 kB)\n",
      "     ------------------------------------- 232.0/232.0 kB 14.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from rich>=10.4.0->keybert) (4.2.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "     ---------------------------------------- 51.1/51.1 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from rich>=10.4.0->keybert) (2.12.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 14.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "     -------------------------------------- 158.0/158.0 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 13.9 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.2/86.2 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.4.24)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "     -------------------------------------- 155.4/155.4 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Building wheels for collected packages: keybert, sentence-transformers\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.5.1-py3-none-any.whl size=21310 sha256=46a31e0b4e29be35bcad56fb17d3b425ecf220769e7f0caa073a5a2cb8ac3c74\n",
      "  Stored in directory: c:\\users\\masilva11\\appdata\\local\\pip\\cache\\wheels\\9a\\87\\f1\\df34ecbb9cd676df6a75511bf922e99bce7d21956a2e0af5ab\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120735 sha256=45f9b61f3b76bd2e9077df228e030f1c6368e9f427383f637b213e5af51b50b5\n",
      "  Stored in directory: c:\\users\\masilva11\\appdata\\local\\pip\\cache\\wheels\\0c\\b6\\fb\\2289a932c365293ad865fc1fe9d2db694d5584241c6d670874\n",
      "Successfully built keybert sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, commonmark, torch, threadpoolctl, rich, pyyaml, filelock, torchvision, scikit-learn, huggingface-hub, transformers, sentence-transformers, keybert\n",
      "Successfully installed commonmark-0.9.1 filelock-3.7.0 huggingface-hub-0.7.0 keybert-0.5.1 pyyaml-6.0 rich-12.4.4 scikit-learn-1.1.1 sentence-transformers-2.2.0 sentencepiece-0.1.96 threadpoolctl-3.1.0 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cobras', 0.5252), ('expeditionary', 0.4148), ('infantry', 0.414), ('snakes', 0.3573), ('army', 0.3554)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "         the learning algorithm to generalize from the training data to unseen situations in a \n",
    "         'reasonable' way (see inductive bias).\"\"\"\n",
    "\n",
    "file = open('bases/brazilian_expeditionary_force.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(contents[0]) # (doc)\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yet Another Keyword Extractor (Yake)\n",
    "#### Referência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yake\n",
      "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "     ---------------------------------------- 60.2/60.2 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake) (2.8.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake) (1.22.4)\n",
      "Requirement already satisfied: click>=6.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake) (8.1.3)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting segtok\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-0.9.0-cp38-cp38-win_amd64.whl (26 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from click>=6.0->yake) (0.4.4)\n",
      "Requirement already satisfied: regex in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from segtok->yake) (2022.4.24)\n",
      "Installing collected packages: tabulate, segtok, jellyfish, yake\n",
      "Successfully installed jellyfish-0.9.0 segtok-1.5.11 tabulate-0.8.9 yake-0.4.8\n"
     ]
    }
   ],
   "source": [
    "!pip3 install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ancient Greek kingdom', 0.0011432268752003777)\n",
      "('III of Macedon', 0.0023938757002022674)\n",
      "('Alexander III', 0.00480735544555275)\n",
      "('kingdom of Macedon', 0.005118679868218627)\n",
      "('ancient Greek', 0.00838103214741652)\n",
      "('Greek kingdom', 0.00838103214741652)\n",
      "('Alexander the Great', 0.011118623211060306)\n",
      "('Macedon', 0.01912649948696399)\n",
      "('Northeastern Africa', 0.032117153871919095)\n",
      "('Western Asia', 0.03656878840611943)\n",
      "('Asia and Northeastern', 0.03656878840611943)\n",
      "('Alexander', 0.038317373296714696)\n",
      "('Alexandros', 0.04585671358294129)\n",
      "('July', 0.04585671358294129)\n",
      "('June', 0.04585671358294129)\n",
      "('Great', 0.0540823928438563)\n",
      "('ruling years conducting', 0.05835888174705578)\n",
      "('lengthy military campaign', 0.060608949093466216)\n",
      "('III', 0.06243062790036526)\n",
      "('Greek', 0.06243062790036526)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "         the learning algorithm to generalize from the training data to unseen situations in a \n",
    "         'reasonable' way (see inductive bias).\"\"\"\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(contents[0]) # (doc)\n",
    "\n",
    "for kw in keywords:\n",
    "  print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction process in Python with Natural Language Processing(NLP)\n",
    "#### Referência: https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Python, Cython, MIT, Matthew Honnibal, Ines Montani, Explosion)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "\n",
    "text = \"\"\"spaCy is an open-source software library for advanced natural language processing, \n",
    "written in the programming languages Python and Cython. The library is published under the MIT license\n",
    "and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\"\n",
    "doc = nlp(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('programming languages Python', 0.001295347548560416)\n",
      "('natural language processing', 0.002012136772192602)\n",
      "('advanced natural language', 0.0026621455770583914)\n",
      "('Python and Cython', 0.0035840985079775055)\n",
      "('open-source software library', 0.008298152696966859)\n",
      "('languages Python', 0.009390717577572831)\n",
      "('language processing', 0.01453240965208459)\n",
      "('software company Explosion', 0.015993140254256993)\n",
      "('advanced natural', 0.01840251352140607)\n",
      "('natural language', 0.019161829017826378)\n",
      "('programming languages', 0.019161829017826378)\n",
      "('open-source software', 0.032652195076937375)\n",
      "('Ines Montani', 0.03375876229391358)\n",
      "('Matthew Honnibal', 0.04096703831447956)\n",
      "('Honnibal and Ines', 0.04096703831447956)\n",
      "('Cython', 0.053691021027863564)\n",
      "('software library', 0.05857047036380304)\n",
      "('company Explosion', 0.06120870235178475)\n",
      "('Python', 0.06651575167590484)\n",
      "('library for advanced', 0.07441175006256819)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "text = \"\"\"spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\"\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5º Artigo/Vídeos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlist Zurubabel\n",
    "#### Referências: \n",
    "<ul>\n",
    "    <li> Playlist - https://www.youtube.com/watch?v=1pGe5OSbbDg&list=PL4OAe-tL47saZwtt9fLHmT5cas57rjCCW&index=1 </li>\n",
    "    <li> Documentação spaCy - https://spacy.io/ </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1923068775.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [3]\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m spacy download pt_core_news_sm\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Instalando pacote/modelo - PTBR.\n",
    "python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\masilva11\\Anaconda3\\envs\\estudos_nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E941] Can't find model 'pt'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"pt_core_news_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"pt\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\masilva11\\Documents\\Pessoal\\Estudos\\internet\\nlp\\estudo_nlp.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/masilva11/Documents/Pessoal/Estudos/internet/nlp/estudo_nlp.ipynb#ch0000047?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/masilva11/Documents/Pessoal/Estudos/internet/nlp/estudo_nlp.ipynb#ch0000047?line=2'>3</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/masilva11/Documents/Pessoal/Estudos/internet/nlp/estudo_nlp.ipynb#ch0000047?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(nlp)\n",
      "File \u001b[1;32mc:\\Users\\masilva11\\Anaconda3\\envs\\estudos_nlp\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=30'>31</a>\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=31'>32</a>\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=35'>36</a>\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=36'>37</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=37'>38</a>\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=38'>39</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=39'>40</a>\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=48'>49</a>\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=49'>50</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=51'>52</a>\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[0;32m     <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/__init__.py?line=52'>53</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\masilva11\\Anaconda3\\envs\\estudos_nlp\\lib\\site-packages\\spacy\\util.py:426\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/util.py?line=423'>424</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/util.py?line=424'>425</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m--> <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/util.py?line=425'>426</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/masilva11/Anaconda3/envs/estudos_nlp/lib/site-packages/spacy/util.py?line=426'>427</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E941] Can't find model 'pt'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"pt_core_news_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"pt\")"
     ]
    }
   ],
   "source": [
    "## Importando biblioteca.\n",
    "import spacy\n",
    "\n",
    "## Carregando modelo.\n",
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando modelos spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baixando modelos em Português.\n",
    "# !python -m spacy download pt_core_news_sm    ## pequeno.\n",
    "# !python -m spacy download pt_core_news_md    ## médio.\n",
    "# !python -m spacy download pt_core_news_lg    ## grande.\n",
    "# !python3 -m spacy download pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpezaTexto(text):\n",
    "    '''\n",
    "        Função faz a limpeza dos textos removendo pontuações e stopwords. Também faz a tokenização e filtragem de acordo com as classes gramaticais.\n",
    "        text: Coluna com dados de texto à serem tratados (pd.series).\n",
    "        Retorno: Textos pós-tratamento (list).\n",
    "    '''\n",
    "\n",
    "    result = []\n",
    "\n",
    "    ## Lista de classes gramaticais.\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN']    ## PROPN: substantivo próprio, ADJ: adjetivo e NOUN: substantivo.\n",
    "    doc = nlp(text.lower())    ## Tokenizando texto.\n",
    "\n",
    "    ## Tratamento do texto.\n",
    "    for token in doc:\n",
    "\n",
    "            ## Se o token for uma stopword ou pontuação deve ser ignorada.\n",
    "            if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "                continue\n",
    "\n",
    "            ## Se o token estiver de acordo com a classi gramatical, deve ser armazernada na lista.\n",
    "            if(token.pos_ in pos_tag):\n",
    "                result.append(token.text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "def calculaTF(dic_de_cont, doc):\n",
    "    '''\n",
    "        Função faz o cálculo da frequência dos termos sobre a quantidade de vezes que cada termo (palavra) aparece no documento.\n",
    "        dic_de_cont: Coluna com dicionario contendo: palavra e a quantidade (pd.series).\n",
    "        doc: Coluna com lista de palavras no documento (pd.series).\n",
    "        Retorno: Dicionário de palavras e o valor TF (dict).\n",
    "    '''\n",
    "\n",
    "    tf_dic = {}\n",
    "    num_palavras_doc = len(doc)    \n",
    "    for palavra, contagem in dic_de_cont.items():     \n",
    "\n",
    "        tf_dic[palavra] = contagem/float(num_palavras_doc)     \n",
    "\n",
    "    return(tf_dic)\n",
    "\n",
    "def calculaIDF(lista_docs):\n",
    "    '''\n",
    "        Função faz o cálculo do inverso da frequência dos termos sobre a quantidade de vezes que cada termo (palavra) aparece em todos os documentos.\n",
    "        lista_docs: Coluna com lista de contagem de termos (pd.series).\n",
    "        Retorno: Dicionário de palavras e o valor IDF (dict).\n",
    "    '''\n",
    "\n",
    "    idf_dic = {}\n",
    "    N = len(lista_docs)\n",
    "    for palavra in lista_docs:\n",
    "\n",
    "        num_docs_aparece = 0\n",
    "\n",
    "        if lista_docs[palavra] > 0:\n",
    "            num_docs_aparece += 1\n",
    "\n",
    "        idf_dic[palavra] = math.log10(N / (num_docs_aparece))\n",
    "\n",
    "    return idf_dic\n",
    "\n",
    "def calculaTF_IDF(serieTF, serieIDF, filtroTF_IDF):\n",
    "    '''\n",
    "        Função faz o cálculo entre as métricas TF e IDF por termo (palavra), também faz a filtragem de acordo com a métrica TF-IDF.\n",
    "        serieTF: Coluna com valores TF (pd.series).\n",
    "        serieIDF: Coluna com valores IDF (pd.series).\n",
    "        Retorno: Dicionário com palavras e o valor TF-IDF (dict).\n",
    "    '''\n",
    "\n",
    "    dicio = {}\n",
    "    for palavra in serieTF.keys():\n",
    "        \n",
    "        valor = serieTF[palavra] * serieIDF[palavra]\n",
    "\n",
    "        ## Separando termos de acordo com o valor TF-IDF.\n",
    "        # if valor >= filtroTF_IDF:\n",
    "        #     dicio[palavra] = valor\n",
    "\n",
    "        dicio[palavra] = valor\n",
    "                    \n",
    "    return dicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação - Módulos, bibliotecas e base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_24848\\2616037086.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "## Importando base de dados.\n",
    "os.chdir('/Users/masilva11/Documents/Stefanini/megazord/notebooks/bases')    ## Caminho diretório - Marco.\n",
    "FileList = glob.glob('*.xlsx')\n",
    "\n",
    "df_dados = pd.DataFrame() \n",
    "for file in FileList:\n",
    "    df = pd.read_excel(file).dropna()[['Cargo', 'Descrição Perfil']]\n",
    "    df_dados = df_dados.append(df)\n",
    "\n",
    "## Resetando índice.\n",
    "df_dados.reset_index(drop = True, inplace = True)\n",
    "\n",
    "## TRATAMENTO DE DADOS.\n",
    "# from unidecode import unidecode\n",
    "# import re\n",
    "\n",
    "# lista = []\n",
    "# index = df_dados['Cargo'].index\n",
    "# df_dados['Cargo Tratado'] = \"\"\n",
    "# for ind in index:\n",
    "\n",
    "    # palavra_sem_acento = unidecode(df_dados['Cargo'].iloc[ind])    ## Remover acentos.\n",
    "    # palavra_sem_caractere = re.sub(r'[^a-zA-Z0-9]', ' ', palavra_sem_acento)    ## Removendo caracteres especiais.\n",
    "    # palavra_sem_espaco = \" \".join(re.split(r'\\s+', palavra_sem_caractere))    ## Removendo espaços duplos.\n",
    "\n",
    "    ## Inserindo dados no dataframe.\n",
    "    # lista.append(palavra_sem_espaco)\n",
    "    # df_dados['Cargo Tratado'].iloc[ind] = palavra_sem_espaco\n",
    "\n",
    "## Reinserindo os dados tratados.\n",
    "# df_dados.insert(1, 'Cargo Tratado', lista, True)\n",
    "# display(df_dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "## Carregando modelo.\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "listaComum, tokens = [], []\n",
    "index = df_dados.index\n",
    "for ind in index:\n",
    "\n",
    "    ## Executando função.\n",
    "    palavras = limpezaTexto(df_dados['Descrição Perfil'].iloc[ind])\n",
    "    listaComum.append(dict(Counter(palavras)))\n",
    "    tokens.append(palavras)\n",
    "\n",
    "## Inserindo dados no dataframe.\n",
    "df_dados['tokens'] = tokens\n",
    "df_dados['cont_termos'] = listaComum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "valorTF, valorIDF, valorTF_IDF = [], [], []\n",
    "for indx in index:\n",
    "\n",
    "    ## Executando função de cálculo IF.\n",
    "    valorTF.append(calculaTF(df_dados['cont_termos'].iloc[indx], df_dados['tokens'].iloc[indx]))\n",
    "\n",
    "    ## Executando função de cálculo IDF.\n",
    "    valorIDF.append(calculaIDF(df_dados['cont_termos'].iloc[:].values[indx]))\n",
    "\n",
    "## Inserindo dados no dataframe.\n",
    "df_dados['valor_tf'] = valorTF\n",
    "df_dados['valor_idf'] = valorIDF\n",
    "\n",
    "for indx in index:\n",
    "\n",
    "    ## Executando função de cálculo TF-IDF.\n",
    "    valorTF_IDF.append(calculaTF_IDF(df_dados['valor_tf'].iloc[indx], df_dados['valor_idf'].iloc[indx], 0.05))\n",
    "\n",
    "## Inserindo dados no dataframe.\n",
    "df_dados['valor_tf_idf'] = valorTF_IDF\n",
    "\n",
    "## Visualizando resultados.\n",
    "# display(df_dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cargo</th>\n",
       "      <th>Descrição Perfil</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cont_termos</th>\n",
       "      <th>valor_tf</th>\n",
       "      <th>valor_idf</th>\n",
       "      <th>valor_tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analista de Qualidade em Atendimento</td>\n",
       "      <td>Solucionadora de conflitos.</td>\n",
       "      <td>[solucionadora, conflitos]</td>\n",
       "      <td>{'solucionadora': 1, 'conflitos': 1}</td>\n",
       "      <td>{'solucionadora': 0.5, 'conflitos': 0.5}</td>\n",
       "      <td>{'solucionadora': 0.3010299956639812, 'conflit...</td>\n",
       "      <td>{'solucionadora': 0.1505149978319906, 'conflit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analista de Qualidade na Volkswagen do Brasil</td>\n",
       "      <td>Planejamento na formação de equipes multifunci...</td>\n",
       "      <td>[planejamento, formação, equipes, multifuncion...</td>\n",
       "      <td>{'planejamento': 2, 'formação': 1, 'equipes': ...</td>\n",
       "      <td>{'planejamento': 0.05128205128205128, 'formaçã...</td>\n",
       "      <td>{'planejamento': 1.4913616938342726, 'formação...</td>\n",
       "      <td>{'planejamento': 0.07648008686329603, 'planos'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analista da Qualidade Pleno</td>\n",
       "      <td>Engenheira Civil com experiência em OCP, OCS e...</td>\n",
       "      <td>[engenheira, civil, experiência, ocp, ocs, ind...</td>\n",
       "      <td>{'engenheira': 1, 'civil': 5, 'experiência': 2...</td>\n",
       "      <td>{'engenheira': 0.013333333333333334, 'civil': ...</td>\n",
       "      <td>{'engenheira': 1.7160033436347992, 'civil': 1....</td>\n",
       "      <td>{'civil': 0.11440022290898662, 'construção': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analista de Qualidade -BRF</td>\n",
       "      <td>Médica Veterinária graduada pela Universidade ...</td>\n",
       "      <td>[médica, veterinária, universidade, federal, g...</td>\n",
       "      <td>{'médica': 1, 'veterinária': 1, 'universidade'...</td>\n",
       "      <td>{'médica': 0.00546448087431694, 'veterinária':...</td>\n",
       "      <td>{'médica': 2.0969100130080562, 'veterinária': ...</td>\n",
       "      <td>{'qualidade': 0.1260437712737083}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analista da Qualidade</td>\n",
       "      <td>Auditoria interna, Auditoria de sistema (quali...</td>\n",
       "      <td>[auditoria, interna, auditoria, qualidade, amb...</td>\n",
       "      <td>{'auditoria': 4, 'interna': 1, 'qualidade': 1,...</td>\n",
       "      <td>{'auditoria': 0.3076923076923077, 'interna': 0...</td>\n",
       "      <td>{'auditoria': 1.0, 'interna': 1.0, 'qualidade'...</td>\n",
       "      <td>{'auditoria': 0.3076923076923077, 'interna': 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cargo  \\\n",
       "0           Analista de Qualidade em Atendimento   \n",
       "1  Analista de Qualidade na Volkswagen do Brasil   \n",
       "2                    Analista da Qualidade Pleno   \n",
       "3                     Analista de Qualidade -BRF   \n",
       "4                          Analista da Qualidade   \n",
       "\n",
       "                                    Descrição Perfil  \\\n",
       "0                        Solucionadora de conflitos.   \n",
       "1  Planejamento na formação de equipes multifunci...   \n",
       "2  Engenheira Civil com experiência em OCP, OCS e...   \n",
       "3  Médica Veterinária graduada pela Universidade ...   \n",
       "4  Auditoria interna, Auditoria de sistema (quali...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                         [solucionadora, conflitos]   \n",
       "1  [planejamento, formação, equipes, multifuncion...   \n",
       "2  [engenheira, civil, experiência, ocp, ocs, ind...   \n",
       "3  [médica, veterinária, universidade, federal, g...   \n",
       "4  [auditoria, interna, auditoria, qualidade, amb...   \n",
       "\n",
       "                                         cont_termos  \\\n",
       "0               {'solucionadora': 1, 'conflitos': 1}   \n",
       "1  {'planejamento': 2, 'formação': 1, 'equipes': ...   \n",
       "2  {'engenheira': 1, 'civil': 5, 'experiência': 2...   \n",
       "3  {'médica': 1, 'veterinária': 1, 'universidade'...   \n",
       "4  {'auditoria': 4, 'interna': 1, 'qualidade': 1,...   \n",
       "\n",
       "                                            valor_tf  \\\n",
       "0           {'solucionadora': 0.5, 'conflitos': 0.5}   \n",
       "1  {'planejamento': 0.05128205128205128, 'formaçã...   \n",
       "2  {'engenheira': 0.013333333333333334, 'civil': ...   \n",
       "3  {'médica': 0.00546448087431694, 'veterinária':...   \n",
       "4  {'auditoria': 0.3076923076923077, 'interna': 0...   \n",
       "\n",
       "                                           valor_idf  \\\n",
       "0  {'solucionadora': 0.3010299956639812, 'conflit...   \n",
       "1  {'planejamento': 1.4913616938342726, 'formação...   \n",
       "2  {'engenheira': 1.7160033436347992, 'civil': 1....   \n",
       "3  {'médica': 2.0969100130080562, 'veterinária': ...   \n",
       "4  {'auditoria': 1.0, 'interna': 1.0, 'qualidade'...   \n",
       "\n",
       "                                        valor_tf_idf  \n",
       "0  {'solucionadora': 0.1505149978319906, 'conflit...  \n",
       "1  {'planejamento': 0.07648008686329603, 'planos'...  \n",
       "2  {'civil': 0.11440022290898662, 'construção': 0...  \n",
       "3                  {'qualidade': 0.1260437712737083}  \n",
       "4  {'auditoria': 0.3076923076923077, 'interna': 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dadosii = df_dados.copy()\n",
    "display(df_dadosii[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTES >>>\n",
      "{'planejamento': 0.07648008686329603, 'formação': 0.03824004343164802, 'equipes': 0.03824004343164802, 'multifuncionais': 0.03824004343164802, 'definição': 0.03824004343164802, 'planos': 0.07648008686329603, 'medição': 0.11472013029494406, 'peças': 0.03824004343164802, 'conjuntos': 0.03824004343164802, 'soldados': 0.03824004343164802, 'multiplicador': 0.03824004343164802, 'tecnologias': 0.03824004343164802, 'normas': 0.03824004343164802, 'volkswagen': 0.07648008686329603, 'alemanha': 0.07648008686329603, 'wolfsburg': 0.03824004343164802, 'responsável': 0.03824004343164802, 'execução': 0.03824004343164802, 'instalação': 0.03824004343164802, 'estrutura': 0.03824004343164802, 'relativa': 0.03824004343164802, '2d': 0.03824004343164802, '3d': 0.03824004343164802, 'programas': 0.03824004343164802, 'maquinas': 0.03824004343164802, 'cnc': 0.03824004343164802, 'tridimensionais': 0.03824004343164802, 'relatórios': 0.03824004343164802, 'dimensionais': 0.03824004343164802, 'projetos': 0.07648008686329603, 'brasil': 0.07648008686329603} 31\n",
      "DEPOIS >>>\n",
      "{'planejamento': 0.07648008686329603, 'planos': 0.07648008686329603, 'medição': 0.11472013029494406, 'volkswagen': 0.07648008686329603, 'alemanha': 0.07648008686329603, 'projetos': 0.07648008686329603, 'brasil': 0.07648008686329603} 7\n"
     ]
    }
   ],
   "source": [
    "# print(df_dados.columns)\n",
    "\n",
    "# for i in range(0, 10):\n",
    "    # print(df_dados.loc[i, 'valor_tf_idf']) # ['Cargo', 'cont_termos', 'valor_tf_idf']])\n",
    "    \n",
    "# df_dadosii = df_dados.copy()\n",
    "\n",
    "i = 1 # 20\n",
    "print(\"ANTES >>>\")\n",
    "print(df_dados.loc[i, 'valor_tf_idf'], len(df_dados.loc[i, 'valor_tf_idf']))\n",
    "\n",
    "print(\"DEPOIS >>>\")\n",
    "print(df_dadosii.loc[i, 'valor_tf_idf'], len(df_dadosii.loc[i, 'valor_tf_idf']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting advertools\n",
      "  Downloading advertools-0.13.1-py2.py3-none-any.whl (309 kB)\n",
      "     -------------------------------------- 309.9/309.9 kB 4.8 MB/s eta 0:00:00\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.6.1-py2.py3-none-any.whl (264 kB)\n",
      "     ------------------------------------- 264.3/264.3 kB 15.9 MB/s eta 0:00:00\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-win_amd64.whl (17.9 MB)\n",
      "     --------------------------------------- 17.9/17.9 MB 25.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from advertools) (1.4.2)\n",
      "Collecting twython\n",
      "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pandas->advertools) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pandas->advertools) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pandas->advertools) (2022.1)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting cryptography>=2.0\n",
      "  Using cached cryptography-37.0.2-cp36-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-22.4.0-py3-none-any.whl (3.1 MB)\n",
      "     ---------------------------------------- 3.1/3.1 MB 19.8 MB/s eta 0:00:00\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting pyOpenSSL>=16.2.0\n",
      "  Using cached pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from scrapy->advertools) (62.3.2)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.3.0-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.6/93.6 kB ? eta 0:00:00\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "     ---------------------------------------- 47.6/47.6 kB 2.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.6.0-py3-none-any.whl (10 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Using cached zope.interface-5.4.0-cp38-cp38-win_amd64.whl (210 kB)\n",
      "Collecting lxml>=3.5.0\n",
      "  Downloading lxml-4.9.0-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 3.6/3.6 MB 28.9 MB/s eta 0:00:00\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting requests-oauthlib>=0.4.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from twython->advertools) (2.27.1)\n",
      "Collecting cffi>=1.12\n",
      "  Using cached cffi-1.15.0-cp38-cp38-win_amd64.whl (179 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from parsel>=1.5.0->scrapy->advertools) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests>=2.1.0->twython->advertools) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests>=2.1.0->twython->advertools) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests>=2.1.0->twython->advertools) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests>=2.1.0->twython->advertools) (2.0.12)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 151.5/151.5 kB ? eta 0:00:00\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting attrs>=19.1.0\n",
      "  Using cached attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 74.6/74.6 kB ? eta 0:00:00\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from Twisted>=17.9.0->scrapy->advertools) (4.2.0)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2\n",
      "  Downloading twisted_iocpsupport-1.0.2-cp38-cp38-win_amd64.whl (45 kB)\n",
      "     ---------------------------------------- 45.5/45.5 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from tldextract->scrapy->advertools) (3.7.0)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=12559 sha256=61ac4eb5046c3bf312dd3a63c5400805f70a8e23717feb57d256841a8685a98f\n",
      "  Stored in directory: c:\\users\\masilva11\\appdata\\local\\pip\\cache\\wheels\\3c\\31\\7f\\d7d7b5f0b9bad841ed856138ff0c5ee2bf2e04dbeb413097c8\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: twisted-iocpsupport, PyDispatcher, pyasn1, incremental, constantly, zope.interface, w3lib, queuelib, pycparser, pyasn1-modules, pyarrow, protego, oauthlib, lxml, jmespath, itemadapter, hyperlink, cssselect, attrs, requests-oauthlib, requests-file, parsel, cffi, Automat, twython, Twisted, tldextract, itemloaders, cryptography, service-identity, pyOpenSSL, scrapy, advertools\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.4.0 advertools-0.13.1 attrs-21.4.0 cffi-1.15.0 constantly-15.1.0 cryptography-37.0.2 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.6.0 itemloaders-1.0.4 jmespath-1.0.0 lxml-4.9.0 oauthlib-3.2.0 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.0.0 pyarrow-8.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 queuelib-1.6.2 requests-file-1.5.1 requests-oauthlib-1.3.1 scrapy-2.6.1 service-identity-21.1.0 tldextract-3.3.0 twisted-iocpsupport-1.0.2 twython-3.9.1 w3lib-1.22.0 zope.interface-5.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install advertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n",
      "C:\\Users\\masilva11\\AppData\\Local\\Temp\\ipykernel_20192\\2678796714.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_dados = df_dados.append(df)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "## IMPORTAÇÕES\n",
    "os.chdir('C:/Users/masilva11/Documents/Pessoal/Estudos/internet/nlp/arquivos/base_dados')    ## Entrada de dados.\n",
    "FileList = glob.glob('*.xlsx')\n",
    "\n",
    "df_dados = pd.DataFrame() \n",
    "for file in FileList:\n",
    "    setor = file\n",
    "    df = pd.read_excel(file)\n",
    "    df[\"Setor\"] = setor\n",
    "    df_dados = df_dados.append(df)\n",
    "\n",
    "df_dados.drop(['Nome', 'Localização', 'Linkedin'], axis = 1, inplace = True)    ## Removendo colunas.\n",
    "df_dados.dropna(axis = 0, inplace = True)    ## Removendo dados nulos.\n",
    "df_dados.reset_index(drop = True, inplace = True)    ## Resetando índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import advertools as adv\n",
    "\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "## Importando modelos.\n",
    "nlp = spacy.load(\"pt_core_news_sm\")    ## Português.\n",
    "index = df_dados.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tratamento_textos:\n",
    "\n",
    "    def remocao_numeros(texto):\n",
    "\n",
    "        palavras_reservadas = ['2d', '3d', '3g', '4g', '5w2h', '5s', 'html3', 'html4', 'html5', 'css3']\n",
    "        palavras_numeros, lista_palavras = [], []\n",
    "\n",
    "        for palavra in texto.split(' '):\n",
    "            if palavra in palavras_reservadas: \n",
    "                lista_palavras.append(palavra)\n",
    "\n",
    "            else:\n",
    "                palavras_numeros.append(re.sub(r'[0-9]+', '', palavra))\n",
    "\n",
    "        texto_tratado = palavras_numeros + lista_palavras\n",
    "\n",
    "        return texto_tratado\n",
    "\n",
    "    def remocao_pontuacoes(texto):\n",
    "\n",
    "        texto_tratado = re.sub(r'[^\\w\\s]',' ', texto.text.lower())\n",
    "\n",
    "        return texto_tratado\n",
    "\n",
    "    def remocao_quebras(texto):\n",
    "\n",
    "        texto_tratado = []\n",
    "        for palavra in texto:\n",
    "\n",
    "            texto_tratado.append(re.sub(r'\\n', ' ', palavra))\n",
    "\n",
    "        return texto_tratado\n",
    "\n",
    "    def remocao_stopwords(texto):\n",
    "\n",
    "        texto_tratado = []\n",
    "\n",
    "        for palavra in texto:\n",
    "\n",
    "            if palavra not in nlp.Defaults.stop_words:\n",
    "                texto_tratado.append(palavra)\n",
    "\n",
    "        return texto_tratado\n",
    "\n",
    "    def remocao_espacos(texto):\n",
    "\n",
    "        espacos = ['', ' ', '  ']\n",
    "        lista_temp, texto_tratado = [], []\n",
    "\n",
    "        for i in texto:\n",
    "            if i not in espacos:\n",
    "                lista_temp.append(i)\n",
    "\n",
    "        for j in lista_temp:\n",
    "            texto_tratado.append(j.strip())\n",
    "\n",
    "        return texto_tratado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencia_palavras(texto_posLimpeza):\n",
    "\n",
    "    ## Execução de função de contagem de palavras.\n",
    "    df_temp = adv.word_frequency(texto_posLimpeza) # , rm_words = list(nlp.Defaults.stop_words))\n",
    "\n",
    "    ## Inserindo dados.\n",
    "    dicionario, lista_palavras, lista_contagens = {}, [], []\n",
    "    for palavra, contagem in zip(df_temp['word'], df_temp['abs_freq']):\n",
    "        lista_palavras.append(palavra)\n",
    "        lista_contagens.append(contagem)\n",
    "\n",
    "    dicionario['word'] = lista_palavras\n",
    "    dicionario['abs_fraq'] = lista_contagens\n",
    "\n",
    "    return dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenando dados.\n",
    "df_dados['Concat'] = df_dados['Cargo'].map(str) + ' ' + df_dados['Descrição Perfil'].map(str)    ## TODO - Trazer essa operação no inicio do código.\n",
    "series = df_dados['Concat']    ## Selecionando somente a coluna.\n",
    "\n",
    "resultados = []\n",
    "for ind in index:\n",
    "    doc = nlp(series[ind])    ## Tokenizando texto.\n",
    "\n",
    "    texto_semPontuacao = tratamento_textos.remocao_pontuacoes(doc)\n",
    "    texto_semNumeros = tratamento_textos.remocao_numeros(texto_semPontuacao)\n",
    "    texto_semQuebras = tratamento_textos.remocao_quebras(texto_semNumeros)\n",
    "    texto_semStopwords = tratamento_textos.remocao_stopwords(texto_semQuebras)\n",
    "    texto_semEspacos = tratamento_textos.remocao_espacos(texto_semStopwords)\n",
    "\n",
    "    resultados.append(frequencia_palavras(texto_semEspacos))\n",
    "\n",
    "df_dados['cont_palavras'] = resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucesso\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    setores = df_dados['Setor'].value_counts().keys()\n",
    "\n",
    "    for setor in setores:\n",
    "        palavras_setor = []\n",
    "        sub_df = df_dados[df_dados['Setor'] == setor] # 'Analista de Qualidade.xlsx']\n",
    "        for lista in sub_df['cont_palavras']:\n",
    "            for tag in lista['word']:\n",
    "                palavras_setor.append(''.join(tag))\n",
    "\n",
    "        dataframe = pd.DataFrame(list(set(palavras_setor)), columns = [setor[:-3]])\n",
    "        dataframe.to_excel(setor[:-3] + '_NOVO_NOVO' + '.xlsx')\n",
    "\n",
    "    print(\"Sucesso\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e) # \"Erro - Identificar\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa68b336162a6aed6d6f6f7e78aeacf9ca0fb119217bcd8b34308119656f54e4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
