{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceitos de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que é Processamento de Linguagem Natural (NLP)? | Leonardo Ribeiro - https://www.youtube.com/watch?v=Kaz-osMcdxk\n",
    "\n",
    "#### <b> Anotações </b>\n",
    "\n",
    "<b> O que é NLP? </b> <br>\n",
    "Algoritmo/modelo de aprendizado profundo utilizado na análise/processamento de linguagem natural.\n",
    "\n",
    "Exemplos: Google tradutor, assistentes virtuais (Siri, Alexa, Google assistente).\n",
    "\n",
    "<b> Linguagem natural </b> <br>\n",
    "Linguagem de comunicação entre humanos.\n",
    "\n",
    "<b> NLP </b> <br>\n",
    "Uma área de IA.\n",
    "Objetivo: Fazer com que o computador entenda a linguagem natural.\n",
    "Muito utilizado para responder perguntas e/ou fazer sugestões.\n",
    "\n",
    "<b> Dificuldades em trabalhar com NLP </b>\n",
    "<ul>\n",
    "    <li> Ambiguidade; </li>\n",
    "    <ul>\n",
    "        <li> Contexto; </li>\n",
    "        <ul>\n",
    "            <li> Exemplos: \"Policiais matam homem com uma faca\". \"O banco está próximo (banco (assento) ou banco (instituição financeira))\". </li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    <li> Probabilística (previsão de palavras/termos); </li>\n",
    "    <ul>\n",
    "        <li> Exemplo: \"Vamos nadar na... (pscina, praia, rio, etc)\". </li>\n",
    "    </ul>\n",
    "    <li> Raciocínio sobre o mundo (realidade, acontecimentos sobre o mundo (contexto)). </li>\n",
    "    <ul>\n",
    "        <li> Exemplo: \"Você viu a tragédia de ontem?\".</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<b> Aplicações </b>\n",
    "<ul>\n",
    "    <li> Tradução automática; </li>\n",
    "    <li> Sugestão de palavras; </li>\n",
    "    <li> Correção automática; </li>\n",
    "    <li> Análise de sentimento; </li>\n",
    "    <li> Anúncios; </li>\n",
    "    <li> Sistemas de diálogo. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introdução ao Processamento de Linguagem Natural - https://www.youtube.com/watch?v=ivTIgsLUfc4\n",
    "\n",
    "#### <b> Anotações </b>\n",
    "\n",
    "<b> O que é NLP? </b> <br>\n",
    "NLP é uma área interdisciplinar na linguística com a computação que visa estudar como o computador recebe, interpreta/processa e reproduz a língua natural humana.\n",
    "\n",
    "<b> Usabilidade do NLP </b>\n",
    "<ul>\n",
    "\t<li> Contrução de chatbots; </li>\n",
    "\t<li> Análise de sentimentos; </li>\n",
    "\t<li> Tradução automática; </li>\n",
    "\t<li> Assistentes digitais; </li>\n",
    "\t<li> Corretores / resumos automáticos de texto. </li>\n",
    "</ul>\n",
    "\n",
    "<b> Pré-processamento </b> <br>\n",
    "Limpeza e preparação do texto <br>\n",
    "Pré-processar um texto é preparar ele para ser recebido pela máquina. <br>\n",
    "Esse passo é importante quando usamos algoritmos mais simples de machine learning, e deixa de ser tão necessário com algumas técnicas de deep learning (modelos robustos). <br>\n",
    "Nem toda tarefa requer o mesmo pré-processamento.\n",
    "\n",
    "<img src = \"bases/img_conceitos_nlp_vid_2.jpg\">\n",
    "\n",
    "<b> Feature Extraction </b> <br>\n",
    "Para usarmos um modelo precisamos de features: <b> uma forma estruturada de armazenar informações. </b> Porém, textos são um tipo de dado não estruturado, assim, é difícil para o computador entendê-los e analisá-los. <br>\n",
    "Por isso, realizamos a chamada feature extraction, ou seja, transformamos o texto em uma informação numérica de modo que seja possível utilizá-lo para alimentar um modelo. Umas das maneiras mais populares e simples de fazer isso é com Bag of Words (BoW). <br>\n",
    "\n",
    "<b> Bag of Words </b> <br>\n",
    "O que é? BoW é uma forma de representar o texto de acordo com a ocorrência das palavras nele. O \"saco de palavras\" recebe esse nome porque não leva em conta a ordem ou a estrutura das palavras no texto, apenas se ela aparece ou a frequência com que aparece nele. <br>\n",
    "Por exemplo, se a palava AULA aparece muito num texto, ela se torna mais central e importante para a máquina. Portanto, BoW pode ser um ótimo método para determinar as palavras significativas de um texto com base no número de vezes que ela é usada. <br>\n",
    "\n",
    "<b> Passos para aplicar o BoW </b> <br>\n",
    "<ul>\n",
    "\t<li> Selecionar os dados; </li>\n",
    "\t<li> Gerar o vocabulário; </li>\n",
    "\t<li> Formar vetores a partir do documento. </li>\n",
    "</ul>\n",
    "\n",
    "<b> TFIDF </b> <br>\n",
    "O que é? Cada palavra no documento recebe uma pontuação TF-IDF, feita multiplicando duas métricas diferentes:\n",
    "<ul>\n",
    "\t<li> TF = Term Frquency (a frequência do termo), que mede a frequência com que um termo ocorre num documento; </li>\n",
    "\t<li> IDF = Inverse Document Frequency (Inverso da frequência nos documentos), que mede o quão importante um termo é no contexto de todos os documentos. </li>\n",
    "</ul>\n",
    "\n",
    "<img src = \"bases/img_conceitos_nlp_vid_2.2.jpg\">\n",
    "\n",
    "<b> Modelos </b>\n",
    "Acesso pelo link: https://github.com/turing-usp/conceitos-basicos-NLP <br>\n",
    "Ou pelo diretório: \"C:\\Users\\masilva11\\Documents\\Pessoal\\Estudos\\internet\\nlp\\conceitos-basicos-NLP\" <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aula 15 - Processamento de Linguagem Natural (PLN) | Curso Ciência de Dados e Machine Learning - https://www.youtube.com/watch?v=TjPyYOE_Khs\n",
    "\n",
    "#### <b> Anotações </b>\n",
    "\n",
    "<b> Usabilidade/Exemplos </b> <br>\n",
    "<ul>\n",
    "\t<li> aaa; </li>\n",
    "\t<li> aaa; </li>\n",
    "\t<li> aaa. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vídeo 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> Playlist - Zurubabel </b> <br>\n",
    "<b> Vídeo 01 </b> <br>\n",
    "<i> Configuração de ambiente... </i>\n",
    "\n",
    "<b> Vídeo 02 </b> <br>\n",
    "Aprendizado Supervisionado e Não supervisionado <br>\n",
    "\n",
    "<b> Tipos de aprendizado </b>\n",
    "<ul>\n",
    "\t<li> Supervisionado </li>\n",
    "\t<li> Quando há noção de padrões, ou seja, há uma noção clara dos dados e o \"que é esperado\" pelo modelo (treinamento de modelos). </li>\n",
    "\t<ul>\n",
    "\t\t<li> Exemplo: Elaborar um modelo que consiga identificar o Faustão utilizando fotos dele. Inputs: 5k de fotos do Faustão e 3k de fotos NÃO Faustão. </li>\n",
    "\t</ul>\n",
    "\t<li> Não supervisionado </li>\n",
    "\t<ul>\n",
    "\t\t<li> Quando não há noção de padrões, ou seja, não há noção dos dados e seus tipos e esperamos que o modelo faça a classificação deles (elaboração de padrões entre os dados). </li>\n",
    "\t</ul>\n",
    "</ul>\n",
    "\n",
    "SpaCy é utilizado em modelos de aprendizado supervisionado. Como a questão de texto e linguagem natural possuir n variáveis, é necessario o supervisionamento e validação dos modelos de acordo com os dados utilizados. <br>\n",
    "\n",
    "<b> Vídeo 03 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraia palavras-chave usando spaCy em Python\n",
    "#### Referência:\n",
    "https://ichi.pro/pt/extraia-palavras-chave-usando-spacy-em-python-81931153149887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hotWords(text):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    ## Lista de classes gramaticais.\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN']    ## PROPN -- substantivo próprio, ADJ -- adjetivo e NOUN -- substantivo.\n",
    "    doc = nlp(text.lower())    ## Tokenizando texto.\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "            ## Se o token for uma stop word ou pontuação deve ser ignorada.\n",
    "            if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "                continue\n",
    "\n",
    "            ## Se o token for válido, deve ser armazernada na lista.\n",
    "            if(token.pos_ in pos_tag):\n",
    "                result.append(token.text)\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'years', 'kingdom', 'iii', 'commanders', 'throne', 'undefeated', 'greece', 'battle', 'july', 'bc', 'campaign', 'western', 'great', 'asia', 'father', 'northwestern', 'greatest', 'alexandros', 'largest', 'africa', 'northeastern', 'successful', 'military', 'age', 'empires', 'king', 'ancient', 'alexander', 'history', 'june', 'macedon', 'greek', 'philip', 'india', 'lengthy', 'ii'}\n"
     ]
    }
   ],
   "source": [
    "## Importando bibliotecas\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "## Importando modelos.\n",
    "nlp = spacy.load(\"en_core_web_sm\")    ## Inglês.\n",
    "# nlp = spacy.load(\"pt_core_news_sm\")    ## Português.\n",
    "\n",
    "## Modelos de texto (EN).\n",
    "## alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('bases/alexander_the_great.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "## Modelos de texto (PT-BR).\n",
    "## alexandre_o_grande.txt ## alfredo_o_grande.txt ## forca_expedicionaria_brasileira.txt\n",
    "# file = open('bases/alexandre_o_grande.txt', 'r', encoding = 'utf8')\n",
    "# contents = file.readlines()\n",
    "\n",
    "## Modelo artigo.\n",
    "# text = '''Welcome to Medium! Medium is a publishing platform where people can read important, \\\n",
    "#     insightful stories on the topics that matter most to them and share ideas with the world.'''\n",
    "\n",
    "## Executando função.\n",
    "listResults = get_hotWords(contents[0])\n",
    "\n",
    "## Resultados sem duplicados.\n",
    "listResults = set(listResults)\n",
    "print(listResults)\n",
    "\n",
    "## Transformando resultados em hastags - Classificação baseado em frequência. \n",
    "# hashtags = [('#' + x[0]) for x in Counter(listResults).most_common(5)]\n",
    "# print(' '.join(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four of the easiest and most effective methods to Extract Keywords from a Single Text using Python\n",
    "#### Referência:\n",
    "https://www.analyticsvidhya.com/blog/2022/01/four-of-the-easiest-and-most-effective-methods-of-keyword-extraction-from-a-single-text-using-python/#h2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAKE\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0020025519308588 <br>\n",
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/LIAAD/yake\n",
      "  Cloning https://github.com/LIAAD/yake to c:\\users\\masilva11\\appdata\\local\\temp\\pip-req-build-2llkowm2\n",
      "  Resolved https://github.com/LIAAD/yake to commit 238ae58c5ba39326a96862ee0e9cb817e5958440\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): still running...\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tabulate in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (0.8.9)\n",
      "Requirement already satisfied: click>=6.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (8.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (1.22.4)\n",
      "Requirement already satisfied: segtok in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (1.5.11)\n",
      "Requirement already satisfied: networkx in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (2.8.2)\n",
      "Requirement already satisfied: jellyfish in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake==0.4.8) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from click>=6.0->yake==0.4.8) (0.4.4)\n",
      "Requirement already satisfied: regex in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from segtok->yake==0.4.8) (2022.4.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake 'C:\\Users\\masilva11\\AppData\\Local\\Temp\\pip-req-build-2llkowm2'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole text to be usedn VECTORIZATION OF TEXT USING DATA MINING METHODS, In the text mining tasks, textual representation should be not only efficient but also interpretable, as this enables an understanding of the operational logic underlying the data mining models. Traditional text vectorization methods such as TF-IDF and bag-of-words are effective and characterized by intuitive interpretability, but suffer from the «curse of dimensionality», and they are unable to capture the meanings of words. On the other hand, modern distributed methods effectively capture the hidden semantics, but they are computationally intensive, time-consuming, and uninterpretable. This article proposes a new text vectorization method called Bag of weighted Concepts BoWC that presents a document according to the concepts’ information it contains. The proposed method creates concepts by clustering word vectors (i.e. word embedding) then uses the frequencies of these concept clusters to represent document vectors. To enrich the resulted document representation, a new modified weighting function is proposed for weighting concepts based on statistics extracted from word embedding information. The generated vectors are characterized by interpretability, low dimensionality, high accuracy, and low computational costs when used in data mining tasks. The proposed method has been tested on five different benchmark datasets in two data mining tasks; document clustering and classification, and compared with several baselines, including Bag-of-words, TF-IDF, Averaged GloVe, Bag-of-Concepts, and VLAC. The results indicate that BoWC outperforms most baselines and gives 7% better accuracy on average\n"
     ]
    }
   ],
   "source": [
    "title = \"VECTORIZATION OF TEXT USING DATA MINING METHODS\"\n",
    "text = \"In the text mining tasks, textual representation should be not only efficient but also interpretable, as this enables an understanding of the operational logic underlying the data mining models. Traditional text vectorization methods such as TF-IDF and bag-of-words are effective and characterized by intuitive interpretability, but suffer from the «curse of dimensionality», and they are unable to capture the meanings of words. On the other hand, modern distributed methods effectively capture the hidden semantics, but they are computationally intensive, time-consuming, and uninterpretable. This article proposes a new text vectorization method called Bag of weighted Concepts BoWC that presents a document according to the concepts’ information it contains. The proposed method creates concepts by clustering word vectors (i.e. word embedding) then uses the frequencies of these concept clusters to represent document vectors. To enrich the resulted document representation, a new modified weighting function is proposed for weighting concepts based on statistics extracted from word embedding information. The generated vectors are characterized by interpretability, low dimensionality, high accuracy, and low computational costs when used in data mining tasks. The proposed method has been tested on five different benchmark datasets in two data mining tasks; document clustering and classification, and compared with several baselines, including Bag-of-words, TF-IDF, Averaged GloVe, Bag-of-Concepts, and VLAC. The results indicate that BoWC outperforms most baselines and gives 7% better accuracy on average\"\n",
    "full_text = title +\", \"+ text\n",
    "\n",
    "print(\"The whole text to be usedn\",full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrase:  operational logic underlying : score 0.008502958451052589\n",
      "Keyphrase:  text vectorization methods : score 0.015613284939549285\n",
      "Keyphrase:  text vectorization : score 0.02310717508615897\n",
      "Keyphrase:  Traditional text vectorization : score 0.02325791341228692\n",
      "Keyphrase:  data mining models : score 0.02830809004349318\n",
      "Keyphrase:  data mining tasks : score 0.033863083795882626\n",
      "Keyphrase:  DATA MINING : score 0.03618462463953267\n",
      "Keyphrase:  text mining tasks : score 0.037652251074155374\n",
      "Keyphrase:  enables an understanding : score 0.04036782511075581\n",
      "Keyphrase:  operational logic : score 0.04036782511075581\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "keywords = kw_extractor.extract_keywords(full_text)\n",
    "\n",
    "for kw, v in keywords:\n",
    "  print(\"Keyphrase: \",kw, \": score\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction Methods from Documents in NLP\n",
    "#### Referência: https://www.analyticsvidhya.com/blog/2022/03/keyword-extraction-methods-from-documents-in-nlp/#h2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake algorithm\n",
    "#### Referência: https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.6.2\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 8.6 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.4.24-cp38-cp38-win_amd64.whl (262 kB)\n",
      "     ------------------------------------- 262.1/262.1 kB 16.8 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: click in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (0.4.4)\n",
      "Installing collected packages: regex, joblib, nltk, rake-nltk\n",
      "Successfully installed joblib-1.1.0 nltk-3.7 rake-nltk-1.0.6 regex-2022.4.24\n"
     ]
    }
   ],
   "source": [
    "## INSTALAÇÃO.\n",
    "!pip3 install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11 june',\n",
       " '21 july',\n",
       " 'lengthy military',\n",
       " 'successful military',\n",
       " 'ruling years',\n",
       " 'father philip',\n",
       " 'ancient greek',\n",
       " '336 bc',\n",
       " 'widely considered',\n",
       " 'northwestern india',\n",
       " 'northeastern africa']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "## Modelos de texto (EN).\n",
    "## alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('bases/alexander_the_great.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "## Modelos de texto (PT-BR).\n",
    "## alexandre_o_grande.txt ## alfredo_o_grande.txt ## forca_expedicionaria_brasileira.txt\n",
    "# file = open('bases/alexandre_o_grande.txt', 'r', encoding = 'utf8')\n",
    "# contents = file.readlines()\n",
    "\n",
    "## Modelo antigo.\n",
    "# my_text = \"\"\"When it comes to evaluating the performance of keyword extractors, you can use some of the standard metrics in machine learning: accuracy, precision, recall, and F1 score. \\\n",
    "#   However, these metrics don’t reflect partial matches; they only consider the perfect match between an extracted segment and the correct prediction for that tag. \\\n",
    "#   Fortunately, there are some other metrics capable of capturing partial matches. An example of this is ROUGE.\"\"\"\n",
    "\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(contents[0]) # (my_text)    ## Extraindo palavras-chaves do texto.\n",
    "\n",
    "## Ranqueando palavras pela pontuação.\n",
    "rankedList = r.get_ranked_phrases_with_scores()    ## Lista de tuplas.\n",
    "\n",
    "keywordList = []\n",
    "for keyword in rankedList:\n",
    "\n",
    "  keyword_updated = keyword[1].split()\n",
    "  keyword_updated_string = \" \".join(keyword_updated[:2])\n",
    "  keywordList.append(keyword_updated_string)\n",
    "\n",
    "  if(len(keywordList) > 10):\n",
    "    break\n",
    "\n",
    "keywordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting summa\n",
      "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
      "     ---------------------------------------- 54.9/54.9 kB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from summa) (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from scipy>=0.19->summa) (1.22.4)\n",
      "Building wheels for collected packages: summa\n",
      "  Building wheel for summa (setup.py): started\n",
      "  Building wheel for summa (setup.py): finished with status 'done'\n",
      "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54390 sha256=5b47ebadff514820244f67c40315ca2fec67ceae2f22d318ab8819b043370047\n",
      "  Stored in directory: c:\\users\\masilva11\\appdata\\local\\pip\\cache\\wheels\\fd\\6a\\dd\\209eb19d5f2266b9cfd06827539bf70435b0ad5fe8244e52d3\n",
      "Successfully built summa\n",
      "Installing collected packages: summa\n",
      "Successfully installed summa-1.2.0\n",
      "[('methods', 0.2958531418898542), ('method', 0.2958531418898542), ('document', 0.2930064955472458), ('concepts', 0.25972098927238496), ('concept', 0.25972098927238496), ('mining', 0.20425273810869565), ('vectorization', 0.20080655873686834), ('word vectors', 0.18267366210822367), ('computationally', 0.16718186386765665), ('computational', 0.16718186386765665)]\n"
     ]
    }
   ],
   "source": [
    "# TextRank\n",
    "\n",
    "!pip install summa\n",
    "from summa import keywords\n",
    "\n",
    "TR_keywords = keywords.keywords(full_text, scores=True)\n",
    "print(TR_keywords[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificação do artigo\n",
    "#### Aplicando artigo para modelos em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho:  19 ['conhecida também', 'cobra está', 'segunda guerra', '1ª divisão', 'era formada', 'brasil participar', 'mais fácil', 'em suas', 'também batizada', 'rompimento da', 'e um', 'incluídos todos', 'seu lema', 'ofensiva aliada', 'força expedicionária', 'tal força', 'que seria', 'que durante', 'que']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "## Executando único modelo.\n",
    "## Modelos de texto (PT-BR).\n",
    "arquivo = open('bases/forca_expedicionaria_brasileira.txt', 'r', encoding = 'utf8')    ## alexandre_o_grande.txt ## alfredo_o_grande.txt ## forca_expedicionaria_brasileira.txt\n",
    "conteudo = arquivo.readlines()\n",
    "\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(conteudo[0])    ## Extraindo palavras-chaves do texto.\n",
    "\n",
    "## Ranqueando palavras pela pontuação.\n",
    "rankedList = r.get_ranked_phrases_with_scores()    ## Lista de tuplas.\n",
    "\n",
    "keywordList = []\n",
    "for keyword in rankedList:\n",
    "\n",
    "  keyword_updated = keyword[1].split()\n",
    "  keyword_updated_string = \" \".join(keyword_updated[:2])\n",
    "  keywordList.append(keyword_updated_string)\n",
    "\n",
    "  ## Filtrando tamanho da lista de palavras.\n",
    "  # if(len(keywordList) > 10):\n",
    "    # break\n",
    "\n",
    "print(\"Tamanho: \", len(keywordList), keywordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "#### Referência: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wife',\n",
       " 'c.',\n",
       " 'anglo',\n",
       " 'england',\n",
       " 'military',\n",
       " 'change',\n",
       " 'aethelwulf',\n",
       " 'brothers',\n",
       " 'alfred',\n",
       " 'turn']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_hotwords(text):\n",
    "\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "\n",
    "    return result\n",
    "\n",
    "## Importando modelos.\n",
    "## alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('bases/alfred_the_great.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "## Modelo antigo.\n",
    "new_text = \"\"\"When it comes to evaluating the performance of keyword extractors, you can use some of the standard metrics in machine learning: accuracy, precision, recall, and F1 score. \\\n",
    "  However, these metrics don’t reflect partial matches; they only consider the perfect match between an extracted segment and the correct prediction for that tag. \\\n",
    "  Fortunately, there are some other metrics capable of capturing partial matches. An example of this is ROUGE.\"\"\"\n",
    "\n",
    "## Executando função.\n",
    "output = set(get_hotwords(contents[0])) ## (new_text))\n",
    "most_common_list = Counter(output).most_common(10)    ## \"Counter\" - Contador de itens. ## \"most_common\" - Ordena os valores pelo tamanho.\n",
    "\n",
    "## Inserindo resultados em lista.\n",
    "result = []\n",
    "for item in most_common_list:\n",
    "  result.append(item[0])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textrank\n",
    "#### Referência: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytextrank\n",
      "  Downloading pytextrank-3.2.3-py3-none-any.whl (30 kB)\n",
      "Collecting scipy>=1.7\n",
      "  Downloading scipy-1.8.1-cp38-cp38-win_amd64.whl (36.9 MB)\n",
      "     ---------------------------------------- 36.9/36.9 MB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments>=2.7.4 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pytextrank) (2.12.0)\n",
      "Collecting icecream>=2.1\n",
      "  Downloading icecream-2.1.2-py2.py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: spacy>=3.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pytextrank) (3.3.0)\n",
      "Collecting graphviz>=0.13\n",
      "  Downloading graphviz-0.20-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 47.0/47.0 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting networkx[default]>=2.6\n",
      "  Downloading networkx-2.8.2-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: executing>=0.3.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from icecream>=2.1->pytextrank) (0.8.3)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from icecream>=2.1->pytextrank) (0.4.4)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from icecream>=2.1->pytextrank) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from networkx[default]>=2.6->pytextrank) (1.22.4)\n",
      "Collecting matplotlib>=3.4\n",
      "  Downloading matplotlib-3.5.2-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 11.2 MB/s eta 0:00:00\n",
      "Collecting pandas>=1.3\n",
      "  Downloading pandas-1.4.2-cp38-cp38-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (8.0.16)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (62.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.7.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.9.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.0.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.2-cp38-cp38-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 55.4/55.4 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "     -------------------------------------- 930.9/930.9 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.1.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "     -------------------------------------- 503.5/503.5 kB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pathy>=0.3.5->spacy>=3.0->pytextrank) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=3.0->pytextrank) (4.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2022.5.18.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from jinja2->spacy>=3.0->pytextrank) (2.1.1)\n",
      "Installing collected packages: pytz, scipy, pillow, networkx, kiwisolver, graphviz, fonttools, cycler, pandas, matplotlib, icecream, pytextrank\n",
      "Successfully installed cycler-0.11.0 fonttools-4.33.3 graphviz-0.20 icecream-2.1.2 kiwisolver-1.4.2 matplotlib-3.5.2 networkx-2.8.2 pandas-1.4.2 pillow-9.1.1 pytextrank-3.2.3 pytz-2022.1 scipy-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytextrank\n",
    "# spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World War II', 'fighter squadron', 'Air Force', 'Cobras Fumantes', 'Força Expedicionária Brasileira', 'Allied forces', 'Expedicionária Brasileira', 'liaison flight', 'the Mediterranean Theatre of World War II', 'Allied']\n"
     ]
    }
   ],
   "source": [
    "import pytextrank\n",
    "import spacy\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "## Importando modelos.\n",
    "## alexander_the_great ## alfred_the_great ## brazilian_expeditionary_force\n",
    "file = open('bases/brazilian_expeditionary_force.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(contents[0]) ## (text)\n",
    "\n",
    "result = []\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases[:10]:\n",
    "    # print(phrase.text)\n",
    "    result.append(phrase.text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBert\n",
    "#### Referência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.5.1.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sentence-transformers>=0.3.8\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "     ---------------------------------------- 79.7/79.7 kB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn>=0.22.2\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-win_amd64.whl (7.3 MB)\n",
      "     ---------------------------------------- 7.3/7.3 MB 15.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from keybert) (1.22.4)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-12.4.4-py3-none-any.whl (232 kB)\n",
      "     ------------------------------------- 232.0/232.0 kB 14.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from rich>=10.4.0->keybert) (4.2.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "     ---------------------------------------- 51.1/51.1 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from rich>=10.4.0->keybert) (2.12.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 14.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "     -------------------------------------- 158.0/158.0 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 13.9 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.2/86.2 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.4.24)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "     -------------------------------------- 155.4/155.4 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Building wheels for collected packages: keybert, sentence-transformers\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.5.1-py3-none-any.whl size=21310 sha256=46a31e0b4e29be35bcad56fb17d3b425ecf220769e7f0caa073a5a2cb8ac3c74\n",
      "  Stored in directory: c:\\users\\masilva11\\appdata\\local\\pip\\cache\\wheels\\9a\\87\\f1\\df34ecbb9cd676df6a75511bf922e99bce7d21956a2e0af5ab\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120735 sha256=45f9b61f3b76bd2e9077df228e030f1c6368e9f427383f637b213e5af51b50b5\n",
      "  Stored in directory: c:\\users\\masilva11\\appdata\\local\\pip\\cache\\wheels\\0c\\b6\\fb\\2289a932c365293ad865fc1fe9d2db694d5584241c6d670874\n",
      "Successfully built keybert sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, commonmark, torch, threadpoolctl, rich, pyyaml, filelock, torchvision, scikit-learn, huggingface-hub, transformers, sentence-transformers, keybert\n",
      "Successfully installed commonmark-0.9.1 filelock-3.7.0 huggingface-hub-0.7.0 keybert-0.5.1 pyyaml-6.0 rich-12.4.4 scikit-learn-1.1.1 sentence-transformers-2.2.0 sentencepiece-0.1.96 threadpoolctl-3.1.0 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cobras', 0.5252), ('expeditionary', 0.4148), ('infantry', 0.414), ('snakes', 0.3573), ('army', 0.3554)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "         the learning algorithm to generalize from the training data to unseen situations in a \n",
    "         'reasonable' way (see inductive bias).\"\"\"\n",
    "\n",
    "file = open('bases/brazilian_expeditionary_force.txt', 'r', encoding = 'utf8')\n",
    "contents = file.readlines()\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(contents[0]) # (doc)\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yet Another Keyword Extractor (Yake)\n",
    "#### Referência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yake\n",
      "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "     ---------------------------------------- 60.2/60.2 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake) (2.8.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake) (1.22.4)\n",
      "Requirement already satisfied: click>=6.0 in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from yake) (8.1.3)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting segtok\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-0.9.0-cp38-cp38-win_amd64.whl (26 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from click>=6.0->yake) (0.4.4)\n",
      "Requirement already satisfied: regex in c:\\users\\masilva11\\anaconda3\\envs\\estudos_nlp\\lib\\site-packages (from segtok->yake) (2022.4.24)\n",
      "Installing collected packages: tabulate, segtok, jellyfish, yake\n",
      "Successfully installed jellyfish-0.9.0 segtok-1.5.11 tabulate-0.8.9 yake-0.4.8\n"
     ]
    }
   ],
   "source": [
    "!pip3 install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ancient Greek kingdom', 0.0011432268752003777)\n",
      "('III of Macedon', 0.0023938757002022674)\n",
      "('Alexander III', 0.00480735544555275)\n",
      "('kingdom of Macedon', 0.005118679868218627)\n",
      "('ancient Greek', 0.00838103214741652)\n",
      "('Greek kingdom', 0.00838103214741652)\n",
      "('Alexander the Great', 0.011118623211060306)\n",
      "('Macedon', 0.01912649948696399)\n",
      "('Northeastern Africa', 0.032117153871919095)\n",
      "('Western Asia', 0.03656878840611943)\n",
      "('Asia and Northeastern', 0.03656878840611943)\n",
      "('Alexander', 0.038317373296714696)\n",
      "('Alexandros', 0.04585671358294129)\n",
      "('July', 0.04585671358294129)\n",
      "('June', 0.04585671358294129)\n",
      "('Great', 0.0540823928438563)\n",
      "('ruling years conducting', 0.05835888174705578)\n",
      "('lengthy military campaign', 0.060608949093466216)\n",
      "('III', 0.06243062790036526)\n",
      "('Greek', 0.06243062790036526)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "         the learning algorithm to generalize from the training data to unseen situations in a \n",
    "         'reasonable' way (see inductive bias).\"\"\"\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(contents[0]) # (doc)\n",
    "\n",
    "for kw in keywords:\n",
    "  print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4º Artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction process in Python with Natural Language Processing(NLP)\n",
    "#### Referência: https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Python, Cython, MIT, Matthew Honnibal, Ines Montani, Explosion)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "\n",
    "text = \"\"\"spaCy is an open-source software library for advanced natural language processing, \n",
    "written in the programming languages Python and Cython. The library is published under the MIT license\n",
    "and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\"\n",
    "doc = nlp(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('programming languages Python', 0.001295347548560416)\n",
      "('natural language processing', 0.002012136772192602)\n",
      "('advanced natural language', 0.0026621455770583914)\n",
      "('Python and Cython', 0.0035840985079775055)\n",
      "('open-source software library', 0.008298152696966859)\n",
      "('languages Python', 0.009390717577572831)\n",
      "('language processing', 0.01453240965208459)\n",
      "('software company Explosion', 0.015993140254256993)\n",
      "('advanced natural', 0.01840251352140607)\n",
      "('natural language', 0.019161829017826378)\n",
      "('programming languages', 0.019161829017826378)\n",
      "('open-source software', 0.032652195076937375)\n",
      "('Ines Montani', 0.03375876229391358)\n",
      "('Matthew Honnibal', 0.04096703831447956)\n",
      "('Honnibal and Ines', 0.04096703831447956)\n",
      "('Cython', 0.053691021027863564)\n",
      "('software library', 0.05857047036380304)\n",
      "('company Explosion', 0.06120870235178475)\n",
      "('Python', 0.06651575167590484)\n",
      "('library for advanced', 0.07441175006256819)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "text = \"\"\"spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\"\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa68b336162a6aed6d6f6f7e78aeacf9ca0fb119217bcd8b34308119656f54e4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
